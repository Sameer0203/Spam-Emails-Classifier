{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d09255115eae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from os import walk\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_FILE = 'SpamData/01_Processing/practice_email.txt'\n",
    "\n",
    "SPAM_1_PATH = 'SpamData/01_Processing/spam_assassin_corpus/spam_1'\n",
    "SPAM_2_PATH = 'SpamData/01_Processing/spam_assassin_corpus/spam_2'\n",
    "EASY_NONSPAM_1_PATH = 'SpamData/01_Processing/spam_assassin_corpus/easy_ham_1'\n",
    "EASY_NONSPAM_2_PATH = 'SpamData/01_Processing/spam_assassin_corpus/easy_ham_2'\n",
    "\n",
    "SPAM_CAT = 1\n",
    "HAM_CAT = 0\n",
    "VOCAB_SIZE = 2500\n",
    "\n",
    "DATA_JSON_FILE = 'SpamData/01_Processing/email-text-data.json'\n",
    "WORD_ID_FILE = 'SpamData/01_Processing/word-by-id.json'\n",
    "\n",
    "TRAINING_DATA_FILE = 'SpamData/02_Training/train_data.txt'\n",
    "TESTING_DATA_FILE = 'SpamData/02_Training/test_data.txt'\n",
    "\n",
    "WHALE_FILE = 'SpamData/01_Processing/wordcloud_resources/whale-icon.png'\n",
    "SKULL_FILE = 'SpamData/01_Processing/wordcloud_resources/skull-icon.png'\n",
    "THUMBS_UP_FILE = 'SpamData/01_Processing/wordcloud_resources/thumbs-up.png'\n",
    "THUMBS_DOWN_FILE = 'SpamData/01_Processing/wordcloud_resources/thumbs-down.png'\n",
    "CUSTOM_FONT_FILE = 'SpamData/01_Processing/wordcloud_resources/OpenSansCondensed-bold.ttf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stream = open(EXAMPLE_FILE, encoding= 'latin-1')\n",
    "message = stream.read()\n",
    "stream.close()\n",
    "\n",
    "print(type(message))\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getfilesystemencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = open(EXAMPLE_FILE, encoding= 'latin-1')\n",
    "\n",
    "is_body = False\n",
    "lines = []\n",
    "\n",
    "for line in stream:\n",
    "    if is_body:\n",
    "        lines.append(line)\n",
    "        \n",
    "    elif line == '\\n':\n",
    "        is_body = True\n",
    "    \n",
    "stream.close()\n",
    "\n",
    "email_body = '\\n'.join(lines)\n",
    "print(email_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_squares(N):\n",
    "    for my_number in range(N):\n",
    "        yield my_number ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in generate_squares(3):\n",
    "    print(i, end= ' ->')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email Body Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_body_generator(path):\n",
    "    \n",
    "    for root, dirnames, filenames in walk(path):\n",
    "        for file_name in filenames:\n",
    "            \n",
    "            filepath = join(root, file_name)\n",
    "        \n",
    "            stream = open(filepath, encoding= 'latin-1')\n",
    "\n",
    "            is_body = False\n",
    "            lines = []\n",
    "\n",
    "            for line in stream:\n",
    "                if is_body:\n",
    "                    lines.append(line)\n",
    "\n",
    "                elif line == '\\n':\n",
    "                    is_body = True\n",
    "\n",
    "            stream.close()\n",
    "\n",
    "            email_body = '\\n'.join(lines)\n",
    "            \n",
    "            yield file_name , email_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_directory(path, classification):\n",
    "    rows = []\n",
    "    row_names = []\n",
    "    \n",
    "    for file_name, email_body in email_body_generator(path):\n",
    "        rows.append({'MESSAGE': email_body, 'CATEGORY': classification})\n",
    "        row_names.append(file_name)\n",
    "        \n",
    "    return pd.DataFrame(rows, index= row_names)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_emails = df_from_directory(SPAM_1_PATH, 1)\n",
    "spam_emails = spam_emails.append(df_from_directory(SPAM_2_PATH, 1))\n",
    "spam_emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_emails.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails = df_from_directory(EASY_NONSPAM_1_PATH, HAM_CAT)\n",
    "ham_emails = ham_emails.append(df_from_directory(EASY_NONSPAM_2_PATH, HAM_CAT))\n",
    "ham_emails.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([spam_emails, ham_emails])\n",
    "print(data.shape)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: Checking For Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any message bodies are null\n",
    "data.MESSAGE.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are empty emials(string length zero)\n",
    "(data.MESSAGE.str.len() == 0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.MESSAGE.str.len() == 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate empty emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data.MESSAGE.str.len() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.MESSAGE.str.len() == 0].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove system file entries from Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('cmds')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add document IDs to track emails in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = range(0, len(data.index))\n",
    "data['DOC_ID'] = document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FILE_NAME'] = data.index\n",
    "data.set_index('DOC_ID', inplace = True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to file using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json(DATA_JSON_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of spam messages visualized (Pie charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.CATEGORY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_of_spam = data.CATEGORY.value_counts()[1]\n",
    "amount_of_ham = data.CATEGORY.value_counts()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Spam', 'Legit Mail']\n",
    "sizes = [amount_of_spam, amount_of_ham]\n",
    "\n",
    "plt.figure(figsize= (2,2), dpi= 227)\n",
    "plt.pie(sizes, labels = category_names, textprops= {'fontsize': 6}, startangle= 0, autopct = '%1.0f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Spam', 'Legit Mail']\n",
    "sizes = [amount_of_spam, amount_of_ham]\n",
    "custom_colors = ['blue', 'red']\n",
    "\n",
    "plt.figure(figsize= (2,2), dpi= 227)\n",
    "plt.pie(sizes, labels = category_names, textprops= {'fontsize': 6}, startangle= 0, \n",
    "        autopct = '%1.0f%%', colors = custom_colors, explode = (0, 0.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Spam', 'Legit Mail']\n",
    "sizes = [amount_of_spam, amount_of_ham]\n",
    "custom_colors = ['blue', 'red']\n",
    "\n",
    "plt.figure(figsize= (2,2), dpi= 227)\n",
    "plt.pie(sizes, labels = category_names, textprops= {'fontsize': 6}, startangle= 0, \n",
    "        autopct = '%1.0f%%', colors = custom_colors, pctdistance = 0.8)\n",
    "centre_circle = plt.Circle((0, 0), radius = 0.6, fc = 'white')\n",
    "plt.gca().add_artist(centre_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Spam', 'Legit Mail', 'updates', 'promotions']\n",
    "sizes = [25, 32, 19, 12]\n",
    "custom_colors = ['blue', 'red', 'green', 'yellow']\n",
    "offset = [0.05, 0.05, 0.05, 0.05]\n",
    "\n",
    "plt.figure(figsize= (2,2), dpi= 227)\n",
    "plt.pie(sizes, labels = category_names, textprops= {'fontsize': 6}, startangle= 0, \n",
    "        autopct = '%1.0f%%', colors = custom_colors, pctdistance = 0.8, explode = offset)\n",
    "centre_circle = plt.Circle((0, 0), radius = 0.6, fc = 'white')\n",
    "plt.gca().add_artist(centre_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre- Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'John is a good boy.'\n",
    "msg.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the NLTK Resources(Tokenizer and stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('shakespeare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'THE John is a good boy.'\n",
    "word_tokenize(msg.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'this' in stop_words: print('found it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'THE John is a good boy. to be not to be'\n",
    "words = word_tokenize(msg.lower())\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "        \n",
    "print(filtered_words)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word stems and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'THE John is a good boy. to be not to be! Nobody expects the Spanish inquisition.'\n",
    "words = word_tokenize(msg.lower())\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        filtered_words.append(stemmed_word)\n",
    "        \n",
    "print(filtered_words)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Panctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'THE John is a good boy. to be not to be! Nobody expects the Spanish inquisition.'\n",
    "words = word_tokenize(msg.lower())\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for word in words:\n",
    "    if word not in stop_words and word.isalpha():\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        filtered_words.append(stemmed_word)\n",
    "        \n",
    "print(filtered_words)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## removing HTML tags from EMAILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[2, 'MESSAGE']\n",
    "soup = BeautifulSoup(data.at[214, 'MESSAGE'], 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for EMAIL processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message(message, stemmer = PorterStemmer(),\n",
    "                 stop_words = set(stopwords.words('english'))):\n",
    "    \n",
    "    words = word_tokenize(message.lower())\n",
    "    \n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            filtered_words.append(stemmer.stem(word))\n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_message(email_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_msg_no_html(message, stemmer = PorterStemmer(),\n",
    "                 stop_words = set(stopwords.words('english'))):\n",
    "    \n",
    "    soup = BeautifulSoup(message, 'html.parser')\n",
    "    cleaned_text = soup.get_text()\n",
    "    words = word_tokenize(cleaned_text.lower())\n",
    "    \n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            filtered_words.append(stemmer.stem(word))\n",
    "            # filtered_words.append(word)\n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_msg_no_html(data.at[2, 'MESSAGE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply cleaning and tokenising to all mesages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing dataframes and series and cleaning subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iat[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[5:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MESSAGE.iloc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_emails = data.MESSAGE.iloc[0:3]\n",
    "\n",
    "nested_list = first_emails.apply(clean_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_list = []\n",
    "#  for sublist in nested_list:\n",
    "#      for item in sublist:\n",
    "#          flat_list.append(item)\n",
    "\n",
    "flat_list = [item for sub_list in nested_list for item in sub_list]\n",
    "        \n",
    "len(flat_list)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nested_list = data.MESSAGE.apply(clean_msg_no_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using logic to slice Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids_spam = data[data.CATEGORY == 1].index\n",
    "doc_ids_ham = data[data.CATEGORY == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list_ham = nested_list.loc[doc_ids_ham]\n",
    "nested_list_spam = nested_list.loc[doc_ids_spam]\n",
    "nested_list_ham.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list_ham.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_ham = [item for sublist in nested_list_ham for item in sublist]\n",
    "normal_words = pd.Series(flat_list_ham).value_counts()\n",
    "\n",
    "print(normal_words.shape[0])\n",
    "normal_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_spam = [item for sublist in nested_list_spam for item in sublist]\n",
    "spammy_words = pd.Series(flat_list_spam).value_counts()\n",
    "\n",
    "print(spammy_words.shape[0])\n",
    "spammy_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud().generate(email_body)\n",
    "plt.imshow(word_cloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_corpus = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "type(example_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [''.join(word) for word in example_corpus]\n",
    "type(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_as_string = ' '.join(word_list)\n",
    "# novel_as_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icon = Image.open(WHALE_FILE)\n",
    "image_mask = Image.new(mode='RGB', size=icon.size, color=(255, 255, 255))\n",
    "image_mask.paste(icon, box= icon)\n",
    "\n",
    "rgb_array = np.array(image_mask) # converts an image object to an array\n",
    "\n",
    "plt.figure(figsize = (16, 8))\n",
    "word_cloud = WordCloud(mask=rgb_array, background_color='white', max_words=400, colormap = 'ocean')\n",
    "word_cloud.generate(novel_as_string)\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_corpus2 = nltk.corpus.shakespeare.words('hamlet.xml')\n",
    "type(example_corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_hamlet = ' '.join(example_corpus2)\n",
    "# novel_hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icon = Image.open(SKULL_FILE)\n",
    "image_mask = Image.new(mode='RGB', size=icon.size, color=(255, 255, 255))\n",
    "image_mask.paste(icon, box= icon)\n",
    "\n",
    "rgb_array = np.array(image_mask) # converts an image object to an array\n",
    "\n",
    "plt.figure(figsize = (16, 8))\n",
    "word_cloud = WordCloud(mask=rgb_array, background_color='white', max_words=400, colormap = 'bone')\n",
    "word_cloud.generate(novel_hamlet)\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud of ham and spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icon = Image.open(THUMBS_UP_FILE)\n",
    "image_mask = Image.new(mode='RGB', size=icon.size, color=(255, 255, 255))\n",
    "image_mask.paste(icon, box= icon)\n",
    "\n",
    "rgb_array = np.array(image_mask) # converts an image object to an array\n",
    "\n",
    "ham_str = ' '.join(flat_list_ham)\n",
    "\n",
    "plt.figure(figsize = (16, 8))\n",
    "word_cloud = WordCloud(mask=rgb_array, background_color='white', max_words=2000, colormap = 'winter')\n",
    "word_cloud.generate(ham_str.upper())\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icon = Image.open(THUMBS_DOWN_FILE)\n",
    "image_mask = Image.new(mode='RGB', size=icon.size, color=(255, 255, 255))\n",
    "image_mask.paste(icon, box= icon)\n",
    "\n",
    "rgb_array = np.array(image_mask) # converts an image object to an array\n",
    "\n",
    "spam_str = ' '.join(flat_list_spam)\n",
    "\n",
    "plt.figure(figsize = (16, 8))\n",
    "word_cloud = WordCloud(mask=rgb_array, background_color='white', max_words=2000, colormap = 'gist_heat', \n",
    "                      max_font_size=300, font_path=CUSTOM_FONT_FILE)\n",
    "word_cloud.generate(spam_str.upper())\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Vocabulary and Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_nested_list = data.MESSAGE.apply(clean_msg_no_html)\n",
    "flat_stemmed_list = [item for sublist in stemmed_nested_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = pd.Series(flat_stemmed_list).value_counts()\n",
    "print('Nr of unique words', unique_words.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words = unique_words[0:VOCAB_SIZE]\n",
    "print('most comman words: \\n', frequent_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocabulary Dataframe with a WORD_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = list(range(0, VOCAB_SIZE))\n",
    "vocab = pd.DataFrame({'VOCAB_WORD': frequent_words.index.values}, index= word_ids)\n",
    "vocab.index.name = 'WORD_ID'\n",
    "vocab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the vocabulary as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.to_csv(WORD_ID_FILE, index_label= vocab.index.name, header=vocab.VOCAB_WORD.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'machin' in set(vocab.VOCAB_WORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email with most number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop\n",
    "clean_email_lengths = []\n",
    "for sublist in stemmed_nested_list:\n",
    "    clean_email_lengths.append(len(sublist))\n",
    "    \n",
    "print('Nr of words in the longest email: ', max(clean_email_lengths))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Email position in the list: ', np.argmax(clean_email_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmed_nested_list[np.argmax(clean_email_lengths)]\n",
    "data.at[np.argmax(clean_email_lengths), 'MESSAGE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features and a sparse matrix\n",
    "\n",
    "### Creating a Data frame with one word per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(stemmed_nested_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(stemmed_nested_list.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_columns_df = pd.DataFrame.from_records(stemmed_nested_list.tolist())\n",
    "word_columns_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the data to training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(word_columns_df, data.CATEGORY, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.index.name = y_train.index.name = 'DOC_ID'\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Sparse matrix for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = pd.Index(vocab.VOCAB_WORD)\n",
    "type(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_matrix(df, indexed_words, labels):\n",
    "    \"\"\"\n",
    "    Returns Sparse matrix as a Data Frame.\n",
    "    \n",
    "    df: A dataframe with words in the columns with a document id as an index (X_train or X_test)\n",
    "    indexed_words: index of words ordered by word_id.\n",
    "    labels: category as a series(y_train)\n",
    "    \"\"\"\n",
    "    \n",
    "    nr_rows = df.shape[0]\n",
    "    nr_cols = df.shape[1]\n",
    "    word_set = set(indexed_words)\n",
    "    dict_list = []\n",
    "    \n",
    "    for i in range(nr_rows):\n",
    "        for j in range(nr_cols):\n",
    "            \n",
    "            word = df.iat[i, j]\n",
    "            if word in word_set:\n",
    "                doc_id = df.index[i]\n",
    "                word_id = indexed_words.get_loc(word)\n",
    "                category = labels.at[doc_id]\n",
    "                \n",
    "                item = {'LABEL': category, 'DOC_ID': doc_id, 'OCCURENCE': 1, 'WORD_ID': word_id}\n",
    "                dict_list.append(item)\n",
    "    \n",
    "    return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sparse_train_df = make_sparse_matrix(X_train, word_index, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine occurances with the pandas groupby() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped = sparse_train_df.groupby(['DOC_ID', 'WORD_ID', 'LABEL']).sum().reset_index()\n",
    "train_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.at[0, 'VOCAB_WORD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Training data as a .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(TRAINING_DATA_FILE, train_grouped, fmt = '%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sparse matrix for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sparse_test_df = make_sparse_matrix(X_test, word_index, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grouped = sparse_test_df.groupby(['DOC_ID', 'WORD_ID', 'LABEL']).sum().reset_index()\n",
    "test_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(TESTING_DATA_FILE, test_grouped, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre - Processing Subtleties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_ids = set(train_grouped.DOC_ID)\n",
    "test_doc_ids = set(test_grouped.DOC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(X_test.index.values) - test_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MESSAGE[134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_msg_no_html(data.MESSAGE[134])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
